{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4d1b897a",
      "metadata": {
        "id": "4d1b897a"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/llm/deepseek.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e33dced-e587-4397-81b3-d6606aa1738a",
      "metadata": {
        "id": "2e33dced-e587-4397-81b3-d6606aa1738a"
      },
      "source": [
        "# DeepSeek\n",
        "\n",
        "# LlamaIndex Llms Integration: DeepSeek\n",
        "\n",
        "This is the DeepSeek integration for LlamaIndex. Visit [DeepSeek](https://api-docs.deepseek.com/) for information on how to get an API key and which models are supported.\n",
        "\n",
        "At the time of writing, you can use:\n",
        "- `deepseek-chat`\n",
        "- `deepseek-reasoner`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5863dde9-84a0-4c33-ad52-cc767442f63f",
      "metadata": {
        "id": "5863dde9-84a0-4c33-ad52-cc767442f63f"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "833bdb2b",
      "metadata": {
        "id": "833bdb2b"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install LlamaIndex 🦙."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4aff387e",
      "metadata": {
        "id": "4aff387e",
        "outputId": "7f55bef9-12d6-4823-ce53-7eac49c1fe13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index-llms-deepseek\n",
            "  Downloading llama_index_llms_deepseek-0.1.1-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting llama-index-llms-openai-like<0.4.0,>=0.3.3 (from llama-index-llms-deepseek)\n",
            "  Downloading llama_index_llms_openai_like-0.3.3-py3-none-any.whl.metadata (751 bytes)\n",
            "Collecting llama-index-core<0.13.0,>=0.12.0 (from llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek)\n",
            "  Downloading llama_index_core-0.12.19-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-index-llms-openai<0.4.0,>=0.3.9 (from llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek)\n",
            "  Downloading llama_index_llms_openai-0.3.20-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.37.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (4.48.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (2.0.38)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (3.11.12)\n",
            "Collecting dataclasses-json (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (1.2.18)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (2.10.6)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (9.0.0)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (4.12.2)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (1.17.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-openai<0.4.0,>=0.3.9->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (1.61.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (0.5.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (1.18.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (1.4.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai<0.4.0,>=0.3.9->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai<0.4.0,>=0.3.9->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai<0.4.0,>=0.3.9->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai<0.4.0,>=0.3.9->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek) (3.1.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like<0.4.0,>=0.3.3->llama-index-llms-deepseek)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Downloading llama_index_llms_deepseek-0.1.1-py3-none-any.whl (2.8 kB)\n",
            "Downloading llama_index_llms_openai_like-0.3.3-py3-none-any.whl (3.1 kB)\n",
            "Downloading llama_index_core-0.12.19-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_llms_openai-0.3.20-py3-none-any.whl (15 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: filetype, dirtyjson, mypy-extensions, marshmallow, typing-inspect, tiktoken, dataclasses-json, llama-index-core, llama-index-llms-openai, llama-index-llms-openai-like, llama-index-llms-deepseek\n",
            "Successfully installed dataclasses-json-0.6.7 dirtyjson-1.0.8 filetype-1.2.0 llama-index-core-0.12.19 llama-index-llms-deepseek-0.1.1 llama-index-llms-openai-0.3.20 llama-index-llms-openai-like-0.3.3 marshmallow-3.26.1 mypy-extensions-1.0.0 tiktoken-0.9.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "%pip install llama-index-llms-deepseek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad297f19-998f-4485-aa2f-d67020058b7d",
      "metadata": {
        "id": "ad297f19-998f-4485-aa2f-d67020058b7d"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.deepseek import DeepSeek\n",
        "\n",
        "# you can also set DEEPSEEK_API_KEY in your environment variables\n",
        "llm = DeepSeek(model=\"deepseek-reasoner\", api_key=\"sk-b5375f930a6e40099b6636e541f03995\")\n",
        "\n",
        "# You might also want to set deepseek as your default llm\n",
        "# from llama_index.core import Settings\n",
        "# Settings.llm = llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d61b10bb-e911-47fb-8e84-19828cf224be",
      "metadata": {
        "id": "d61b10bb-e911-47fb-8e84-19828cf224be"
      },
      "outputs": [],
      "source": [
        "response = llm.complete(\"Is 9.9 or 9.11 bigger?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bd14f4e-c245-4384-a471-97e4ddfcb40e",
      "metadata": {
        "id": "3bd14f4e-c245-4384-a471-97e4ddfcb40e",
        "outputId": "32a07089-2c29-45f6-f72a-1390a6db5c08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To determine whether 9.9 or 9.11 is larger, compare them by aligning their decimal places:\n",
            "\n",
            "1. **Write both numbers with the same number of decimal places**:  \n",
            "   - \\(9.9\\) becomes \\(9.90\\).  \n",
            "   - \\(9.11\\) remains \\(9.11\\).  \n",
            "\n",
            "2. **Compare digit by digit**:  \n",
            "   - **Units place**: Both have \\(9\\) (equal).  \n",
            "   - **Tenths place**: \\(9\\) (in \\(9.90\\)) vs. \\(1\\) (in \\(9.11\\)). Since \\(9 > 1\\), \\(9.90 > 9.11\\).  \n",
            "\n",
            "**Conclusion**:  \n",
            "\\(9.9\\) (or \\(9.90\\)) is greater than \\(9.11\\).  \n",
            "\n",
            "\\(\\boxed{9.9}\\)\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ba9503c-b440-43c6-a50c-676c79993813",
      "metadata": {
        "id": "3ba9503c-b440-43c6-a50c-676c79993813"
      },
      "source": [
        "#### Call `chat` with a list of messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee8a4a55-5680-4dc6-a44c-fc8ad7892f80",
      "metadata": {
        "id": "ee8a4a55-5680-4dc6-a44c-fc8ad7892f80"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(\n",
        "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
        "    ),\n",
        "    ChatMessage(\n",
        "        role=\"user\", content=\"How many 'r's are in the word 'strawberry'?\"\n",
        "    ),\n",
        "]\n",
        "resp = llm.chat(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a9bfe53-d15b-4e75-9d91-8c5d024f4eda",
      "metadata": {
        "id": "2a9bfe53-d15b-4e75-9d91-8c5d024f4eda",
        "outputId": "6fd3eb7d-a5bb-471a-bc11-a778e1198d57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant: Arrr, matey! Let's plunder the word \"strawberry\" fer them sneaky 'r's! Here be the breakdown:  \n",
            "\n",
            "**S - T - R - A - W - B - E - R - R - Y**  \n",
            "\n",
            "Shiver me timbers! There be **3 'r's** lurkin' in them letters! Aye, one in \"straw\" and two in \"berry\"—just like treasure buried in three chests! 🏴☠️🍓\n"
          ]
        }
      ],
      "source": [
        "print(resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25ad1b00-28fc-4bcd-96c4-d5b35605721a",
      "metadata": {
        "id": "25ad1b00-28fc-4bcd-96c4-d5b35605721a"
      },
      "source": [
        "### Streaming"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13c641fa-345a-4dce-87c5-ab1f6dcf4757",
      "metadata": {
        "id": "13c641fa-345a-4dce-87c5-ab1f6dcf4757"
      },
      "source": [
        "Using `stream_complete` endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06da1ef1-2f6b-497c-847b-62dd2df11491",
      "metadata": {
        "id": "06da1ef1-2f6b-497c-847b-62dd2df11491"
      },
      "outputs": [],
      "source": [
        "response = llm.stream_complete(\"Is 9.9 or 9.11 bigger?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b851def-5160-46e5-a30c-5a3ef2356b79",
      "metadata": {
        "id": "1b851def-5160-46e5-a30c-5a3ef2356b79",
        "outputId": "ac8aacef-ae3b-4bc2-fa81-bac088c61098"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To determine whether 9.9 or 9.11 is bigger, we can compare them by converting both numbers to have the same number of decimal places. \n",
            "\n",
            "- 9.9 can be written as 9.90 (adding a zero to make it two decimal places).\n",
            "- 9.11 is already in two decimal places.\n",
            "\n",
            "Next, we compare the tenths place:\n",
            "- 9.90 has a 9 in the tenths place.\n",
            "- 9.11 has a 1 in the tenths place.\n",
            "\n",
            "Since 9 is greater than 1, 9.90 is larger than 9.11. \n",
            "\n",
            "To confirm, we can subtract:\n",
            "\\[ 9.90 - 9.11 = 0.79 \\]\n",
            "The positive result indicates that 9.90 is greater than 9.11.\n",
            "\n",
            "Another method is converting to fractions:\n",
            "- 9.9 is \\( \\frac{99}{10} \\) which is equivalent to \\( \\frac{990}{100} \\).\n",
            "- 9.11 is \\( \\frac{911}{100} \\).\n",
            "\n",
            "Comparing \\( \\frac{990}{100} \\) and \\( \\frac{911}{100} \\), we see 990 is greater than 911.\n",
            "\n",
            "Thus, the larger number is \\boxed{9.9}."
          ]
        }
      ],
      "source": [
        "for r in response:\n",
        "    print(r.delta, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca52051d-6b28-49d7-98f5-82e266a1c7a6",
      "metadata": {
        "id": "ca52051d-6b28-49d7-98f5-82e266a1c7a6"
      },
      "source": [
        "Using `stream_chat` endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe553190-52a9-436d-84ae-4dd99a1808f4",
      "metadata": {
        "id": "fe553190-52a9-436d-84ae-4dd99a1808f4"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(\n",
        "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
        "    ),\n",
        "    ChatMessage(\n",
        "        role=\"user\", content=\"How many 'r's are in the word 'strawberry'?\"\n",
        "    ),\n",
        "]\n",
        "resp = llm.stream_chat(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "154c503c-f893-4b6b-8a65-a9a27b636046",
      "metadata": {
        "id": "154c503c-f893-4b6b-8a65-a9a27b636046",
        "outputId": "32c6bd75-026e-40af-e40c-118275d25eff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Arrr, matey! Let's plunder the letters o' \"strawberry\" to count them sneaky 'r's! 🏴☠️\n",
            "\n",
            "**S-T-R-A-W-B-E-R-R-Y**  \n",
            "Yarrr, here be the breakdown:  \n",
            "\n",
            "1. **S** 🚫  \n",
            "2. **T** 🚫  \n",
            "3. **R** ✅ (1st 'r')  \n",
            "4. **A** 🚫  \n",
            "5. **W** 🚫  \n",
            "6. **B** 🚫  \n",
            "7. **E** 🚫  \n",
            "8. **R** ✅ (2nd 'r')  \n",
            "9. **R** ✅ (3rd 'r')  \n",
            "10. **Y** 🚫  \n",
            "\n",
            "**Total 'r's: 3**  \n",
            "Shiver me timbers! Three 'r's be lurkin' in \"strawberry\"! 🍓⚔️"
          ]
        }
      ],
      "source": [
        "for r in resp:\n",
        "    print(r.delta, end=\"\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}